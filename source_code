import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Load the dataset
columns = ['id', 'account', 'Label', 'Text']
df = pd.read_csv("Downloads/twitter_training.csv", names=columns)

# Preprocess the dataset
df.dropna(inplace=True)  # Remove missing values
df.drop_duplicates(inplace=True)  # Remove duplicates

# Encode the labels
encode = LabelEncoder()
df['Label_transform'] = encode.fit_transform(df['Label'])

# Extract features and labels
X = df['Text']
y = df['Label_transform']
print(df.columns)

from transformers import BertTokenizer
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize the dataset
inputs = tokenizer(list(X), padding=True, truncation=True, max_length=128, return_tensors="pt")
labels = torch.tensor(y.values)
from transformers import BertForSequenceClassification, AdamW
from torch.utils.data import DataLoader, TensorDataset
import torch

# Prepare the dataset for training
dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)
train_loader = DataLoader(dataset, batch_size=16, shuffle=True)

# Load the BERT model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(encode.classes_))
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Set up optimizer
optimizer = AdamW(model.parameters(), lr=5e-5)

# Training loop
model.train()
epochs = 3

for epoch in range(epochs):
    for batch in train_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        
        # Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        
        # Optimizer step
        optimizer.step()
        optimizer.zero_grad()
    
    print(f"Epoch {epoch+1}/{epochs} completed. Loss: {loss.item()}")
    
